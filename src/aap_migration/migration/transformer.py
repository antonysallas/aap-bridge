"""Data transformers for source-to-target AAP compatibility.

This module provides transformation logic to convert source AAP resource data
to target AAP compatible format, handling deprecated fields, new requirements,
and structural changes.
"""

from __future__ import annotations

import json
import secrets
from copy import deepcopy
from pathlib import Path
from typing import TYPE_CHECKING, Any

from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization
from cryptography.hazmat.primitives.asymmetric import rsa

from aap_migration.config import MigrationConfig
from aap_migration.schema.models import ComparisonResult
from aap_migration.utils.logging import get_logger

if TYPE_CHECKING:
    from aap_migration.migration.state import MigrationState

logger = get_logger(__name__)


def generate_temp_ssh_key() -> str:
    """Generate a temporary RSA private key in PEM format.

    This creates a valid but disposable SSH private key for API validation.
    Users must replace this with real credentials after migration.

    Returns:
        PEM-formatted RSA private key string
    """
    private_key = rsa.generate_private_key(
        public_exponent=65537,
        key_size=2048,
        backend=default_backend(),
    )
    pem = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.TraditionalOpenSSL,
        encryption_algorithm=serialization.NoEncryption(),
    )
    return pem.decode("utf-8")


def generate_temp_encrypted_ssh_key(passphrase: str) -> str:
    """Generate a temporary RSA private key encrypted with a passphrase.

    This is required when ssh_key_unlock is set, as the API validates
    that the key is actually encrypted.

    Args:
        passphrase: The passphrase to encrypt the key with

    Returns:
        PEM-formatted encrypted RSA private key string
    """
    private_key = rsa.generate_private_key(
        public_exponent=65537,
        key_size=2048,
        backend=default_backend(),
    )
    pem = private_key.private_bytes(
        encoding=serialization.Encoding.PEM,
        format=serialization.PrivateFormat.TraditionalOpenSSL,
        encryption_algorithm=serialization.BestAvailableEncryption(
            passphrase.encode("utf-8")
        ),
    )
    return pem.decode("utf-8")


# =============================================================================
# Exceptions
# =============================================================================


class SkipResourceError(Exception):
    """Raised when a resource should be skipped during transformation.

    This occurs when a resource's required dependencies were not exported
    (e.g., host's inventory was pending_deletion, or a credential references
    a non-exported organization).

    Attributes:
        resource_type: Type of resource being transformed
        source_id: Source system resource ID
        missing_dependency: Description of the missing dependency (e.g., "inventories:123")
    """

    def __init__(
        self,
        message: str,
        resource_type: str,
        source_id: int,
        missing_dependency: str,
    ):
        super().__init__(message)
        self.resource_type = resource_type
        self.source_id = source_id
        self.missing_dependency = missing_dependency

class DataTransformer:
    """Base transformer for converting AAP 2.3 data to AAP 2.6 format.

    Handles common transformations:
    - Removing deprecated fields
    - Adding required new fields
    - Converting field formats
    - Applying field renames
    - Validating transformed data
    - Validating dependencies exist in id_mappings (NEW)
    - Registering source mappings during transformation (NEW)

    This transformer is schema-driven, loading field definitions from
    schema_comparison.json generated by the schema comparison system.

    Subclasses should override DEPENDENCIES and REQUIRED_DEPENDENCIES to define
    which fields reference other resource types and which are required vs optional.
    """

    # Read-only fields that should be removed before import (universal)
    READ_ONLY_FIELDS = [
        "id",
        "type",
        "url",
        "related",
        "summary_fields",
        "created",
        "modified",
        "last_job_run",
        "last_update_failed",
        "status",
        "has_active_failures",
        "total_hosts",
        "hosts_with_active_failures",
        "total_groups",
        "groups_with_active_failures",
        # Project-specific read-only fields
        "local_path",       # Auto-generated filesystem path (causes 400 conflicts)
        "scm_revision",     # Current SCM commit hash (updated after sync)
        "last_updated",     # Last update timestamp
        "last_job_failed",  # Last job failure status
        "next_job_run",     # Scheduled next run time
    ]

    # Required fields for each resource type (empty - schema comparison handles this)
    # This attribute is referenced in _add_required_fields() but schema-driven approach
    # uses schema_comparison_data instead of hardcoded values
    REQUIRED_FIELDS: dict[str, dict[str, Any]] = {}

    # ==========================================================================
    # Dependency Configuration (override in subclasses)
    # ==========================================================================

    # Maps field name to resource type for dependency resolution
    # Example: {"organization": "organizations", "inventory": "inventories"}
    DEPENDENCIES: dict[str, str] = {}

    # Fields that are REQUIRED dependencies - if missing, raise SkipResourceError
    # Optional dependencies (in DEPENDENCIES but not here) just log a warning
    REQUIRED_DEPENDENCIES: set[str] = set()

    def __init__(
        self,
        dry_run: bool = False,
        schema_comparison: ComparisonResult | None = None,
        schema_comparison_file: str | Path | None = None,
        state: MigrationState | None = None,
        input_dir: Path | None = None,
        config: MigrationConfig | None = None,
        **kwargs: Any,
    ):
        """Initialize data transformer.

        Args:
            dry_run: If True, log transformations but don't modify data
            schema_comparison: Optional schema comparison result for informed transformations
            schema_comparison_file: Path to schema_comparison.json file
            state: Optional MigrationState for dependency validation and source mapping
            input_dir: Directory containing raw exports (for cross-referencing)
            config: Optional MigrationConfig containing resource mappings
        """
        self.dry_run = dry_run
        self.schema_comparison = schema_comparison
        self.schema_comparison_data: dict[str, Any] | None = None
        self.state = state
        self.input_dir = input_dir
        self.config = config

        # Load schema comparison from file if provided
        if schema_comparison_file:
            schema_path = Path(schema_comparison_file)
            if schema_path.exists():
                with open(schema_path) as f:
                    self.schema_comparison_data = json.load(f)

                # Support both old format ("resources") and new format ("transformations")
                transformations_count = len(
                    self.schema_comparison_data.get("transformations",
                    self.schema_comparison_data.get("resources", {}))
                )

                logger.info(
                    "schema_comparison_loaded",
                    file=str(schema_path),
                    resource_count=transformations_count,
                )
            else:
                logger.warning(
                    "schema_comparison_file_not_found",
                    file=str(schema_path),
                    fallback="Using resource-specific transformations only",
                )

        self.stats = {
            "transformed_count": 0,
            "fields_removed": 0,
            "fields_added": 0,
            "fields_renamed": 0,
            "validation_errors": 0,
            "skipped_count": 0,
        }

    def transform_resource(
        self,
        resource_type: str,
        data: dict[str, Any],
        validate: bool = True,
    ) -> dict[str, Any]:
        """Transform a resource for AAP 2.6 compatibility.

        This method:
        1. Validates that required dependencies exist in id_mappings (raises SkipResourceError if not)
        2. Applies resource-specific transformations
        3. Removes read-only and deprecated fields
        4. Adds required new fields
        5. Registers the resource in id_mappings with target_id=NULL

        Args:
            resource_type: Type of resource (e.g., 'inventories', 'job_templates')
            data: Source resource data from AAP 2.3
            validate: Whether to validate transformed data

        Returns:
            Transformed resource data compatible with AAP 2.6

        Raises:
            SkipResourceError: If a required dependency is missing from id_mappings
        """
        # Create deep copy to avoid modifying original
        transformed = deepcopy(data)

        logger.debug(
            "transforming_resource",
            resource_type=resource_type,
            source_id=data.get("id"),
            source_name=data.get("name"),
        )

        # Step 1: Validate dependencies exist in id_mappings
        # This must happen FIRST, before any transformations
        # Raises SkipResourceError if a required dependency is missing
        self._validate_dependencies(transformed, resource_type)

        # Step 2: Apply transformations in order
        # NOTE: specific transformations run BEFORE read-only and deprecated field removal
        # so they can inspect all fields for migration logic (e.g., extracting IDs from summary_fields)
        transformed = self._apply_specific_transformations(transformed, resource_type)
        transformed = self._remove_read_only_fields(transformed)
        transformed = self._apply_field_renames(transformed, resource_type)
        transformed = self._remove_deprecated_fields(transformed, resource_type)
        transformed = self._add_required_fields(transformed, resource_type)

        # Step 3: Validate transformed data if requested
        if validate:
            self._validate_transformed_data(transformed, resource_type)

        # Step 4: Register source mapping in id_mappings with target_id=NULL
        # This happens AFTER successful transformation so we only track valid resources
        self._register_source_mapping(data, resource_type)

        self.stats["transformed_count"] += 1

        logger.debug(
            "transformation_complete",
            resource_type=resource_type,
            source_id=data.get("id"),
            transformations_applied=True,
        )

        return transformed

    # ==========================================================================
    # Dependency Validation Methods (NEW)
    # ==========================================================================

    def _validate_dependencies(
        self,
        data: dict[str, Any],
        resource_type: str,
    ) -> None:
        """Validate that all required dependencies exist in id_mappings.

        Checks each field defined in DEPENDENCIES against the id_mappings table.
        For required dependencies (in REQUIRED_DEPENDENCIES), raises SkipResourceError
        if the dependency is missing. For optional dependencies, logs a warning.

        Args:
            data: Resource data to validate
            resource_type: Type of resource being transformed

        Raises:
            SkipResourceError: If a required dependency is missing from id_mappings
        """
        if not self.state:
            # No state manager - skip dependency validation
            return

        if not self.DEPENDENCIES:
            # No dependencies defined for this resource type
            return

        source_id = data.get("_source_id") or data.get("id")

        for field, dep_resource_type in self.DEPENDENCIES.items():
            dep_source_id = data.get(field)

            # Skip if field is not set or is None/0
            if not dep_source_id:
                continue

            # Check if the dependency exists in id_mappings
            if not self.state.has_source_mapping(dep_resource_type, dep_source_id):
                if field in self.REQUIRED_DEPENDENCIES:
                    # Required dependency is missing - skip this resource
                    logger.warning(
                        "required_dependency_missing",
                        resource_type=resource_type,
                        source_id=source_id,
                        source_name=data.get("name"),
                        field=field,
                        dep_resource_type=dep_resource_type,
                        dep_source_id=dep_source_id,
                        message=f"Skipping {resource_type} - required {dep_resource_type} not exported",
                    )
                    self.stats["skipped_count"] += 1
                    raise SkipResourceError(
                        f"{resource_type} {source_id} references non-exported "
                        f"{dep_resource_type} {dep_source_id}",
                        resource_type=resource_type,
                        source_id=source_id,
                        missing_dependency=f"{dep_resource_type}:{dep_source_id}",
                    )
                else:
                    # Optional dependency is missing - log warning but continue
                    logger.info(
                        "optional_dependency_missing",
                        resource_type=resource_type,
                        source_id=source_id,
                        source_name=data.get("name"),
                        field=field,
                        dep_resource_type=dep_resource_type,
                        dep_source_id=dep_source_id,
                        message="Optional dependency missing - field will be removed during import",
                    )

    def _register_source_mapping(
        self,
        data: dict[str, Any],
        resource_type: str,
    ) -> None:
        """Register this resource in id_mappings with target_id=NULL.

        Called after successful transformation to track that this resource
        was exported and transformed. The target_id will be populated later
        during the import phase.

        Args:
            data: Original resource data (before transformation)
            resource_type: Type of resource
        """
        if not self.state:
            # No state manager - skip registration
            return

        source_id = data.get("_source_id") or data.get("id")
        source_name = data.get("name")

        if source_id:
            self.state.create_source_mapping(
                resource_type=resource_type,
                source_id=source_id,
                source_name=source_name,
            )

    def _remove_read_only_fields(self, data: dict[str, Any]) -> dict[str, Any]:
        """Remove read-only fields that cannot be set during import.

        Args:
            data: Resource data

        Returns:
            Data with read-only fields removed
        """
        for field in self.READ_ONLY_FIELDS:
            if field in data:
                del data[field]
                self.stats["fields_removed"] += 1

        return data

    def _apply_field_renames(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply field renames from schema comparison.

        Renames fields that have been renamed between AAP 2.3 and 2.6.
        Only applies renames marked as auto_fixable (high/medium confidence).

        Args:
            data: Resource data
            resource_type: Type of resource

        Returns:
            Data with renamed fields
        """
        if not self.schema_comparison_data:
            return data

        # Support both "transformations" (new) and "resources" (old) format
        transformations = self.schema_comparison_data.get(
            "transformations",
            self.schema_comparison_data.get("resources", {})
        )
        resource_schema = transformations.get(resource_type, {})
        field_renames = resource_schema.get("fields_renamed", resource_schema.get("field_renames", {}))

        source_id = data.get("_source_id") or data.get("id")
        for old_name, rename_info in field_renames.items():
            # Support two formats:
            # 1. Dict with auto_fixable, new_name, confidence (old format)
            # 2. String (simple old_name -> new_name mapping from prep)
            if old_name in data:
                if isinstance(rename_info, dict):
                    # Old format with auto_fixable check
                    if rename_info.get("auto_fixable"):
                        new_name = rename_info["new_name"]
                        data[new_name] = data.pop(old_name)
                        self.stats["fields_renamed"] += 1
                        logger.debug(
                            "field_renamed",
                            resource_type=resource_type,
                            source_id=source_id,
                            old_name=old_name,
                            new_name=new_name,
                            confidence=rename_info.get("confidence"),
                        )
                elif isinstance(rename_info, str):
                    # New format: simple string mapping
                    new_name = rename_info
                    data[new_name] = data.pop(old_name)
                    self.stats["fields_renamed"] += 1
                    logger.debug(
                        "field_renamed",
                        resource_type=resource_type,
                        source_id=source_id,
                        old_name=old_name,
                        new_name=new_name,
                    )

        return data

    def _remove_deprecated_fields(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Remove fields deprecated in AAP 2.6.

        Uses schema comparison data to identify deprecated fields for this resource type.

        Args:
            data: Resource data
            resource_type: Type of resource

        Returns:
            Data with deprecated fields removed
        """
        deprecated_fields = []

        # Get deprecated fields from schema comparison data
        if self.schema_comparison_data:
            # Support both "transformations" (new) and "resources" (old) format
            transformations = self.schema_comparison_data.get(
                "transformations",
                self.schema_comparison_data.get("resources", {})
            )
            resource_schema = transformations.get(resource_type, {})
            # New format uses "fields_removed", old format uses "deprecated_fields"
            deprecated_fields = resource_schema.get(
                "fields_removed",
                resource_schema.get("deprecated_fields", [])
            )

        # Also check ComparisonResult object if available
        if self.schema_comparison:
            deprecated_fields.extend(self.schema_comparison.deprecated_fields)

        # Remove duplicates
        deprecated_fields = list(set(deprecated_fields))

        source_id = data.get("_source_id") or data.get("id")
        for field in deprecated_fields:
            if field in data:
                logger.debug(
                    "removing_deprecated_field",
                    resource_type=resource_type,
                    source_id=source_id,
                    field=field,
                )
                del data[field]
                self.stats["fields_removed"] += 1

        return data

    def _add_required_fields(self, data: dict[str, Any], resource_type: str) -> dict[str, Any]:
        """Add fields required by AAP 2.6.

        Uses schema comparison data to identify new required fields for this resource type.

        Args:
            data: Resource data
            resource_type: Type of resource

        Returns:
            Data with required fields added
        """
        required_fields = {}

        # Get required fields from schema comparison data
        if self.schema_comparison_data:
            # Support both "transformations" (new) and "resources" (old) format
            transformations = self.schema_comparison_data.get(
                "transformations",
                self.schema_comparison_data.get("resources", {})
            )
            resource_schema = transformations.get(resource_type, {})
            # New format uses "new_required_defaults", old format uses "new_required_fields"
            required_fields = resource_schema.get(
                "new_required_defaults",
                resource_schema.get("new_required_fields", {})
            )

        # Also check ComparisonResult object if available
        if self.schema_comparison:
            required_fields.update(self.schema_comparison.new_required_fields)

        source_id = data.get("_source_id") or data.get("id")
        for field, default_value in required_fields.items():
            if field not in data:
                logger.debug(
                    "adding_required_field",
                    resource_type=resource_type,
                    source_id=source_id,
                    field=field,
                    default_value=default_value,
                )
                data[field] = default_value
                self.stats["fields_added"] += 1

        return data

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply resource-specific transformations.

        Subclasses override this for custom logic.

        Args:
            data: Resource data
            resource_type: Type of resource

        Returns:
            Transformed data
        """
        # Apply resource-specific transformations based on type
        if resource_type == "credentials":
            return self._transform_credentials(data)
        elif resource_type == "inventories":
            return self._transform_inventories(data)
        elif resource_type == "inventory_groups" or resource_type == "groups":
            return self._transform_inventory_groups(data)
        elif resource_type == "job_templates":
            return self._transform_job_templates(data)
        elif resource_type == "projects":
            return self._transform_projects(data)
        elif resource_type == "inventory_sources":
            return self._transform_inventory_sources(data)
        elif resource_type == "execution_environments":
            return self._transform_execution_environments(data)

        # Base implementation - no specific transformations
        return data

    def _transform_credentials(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply credential-specific transformations.

        Args:
            data: Credential data

        Returns:
            Transformed credential data with encrypted values handled
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract organization ID from summary_fields BEFORE they're removed
        if "summary_fields" in data and "organization" in data["summary_fields"]:
            org_info = data["summary_fields"]["organization"]
            if isinstance(org_info, dict) and "id" in org_info:
                data["organization"] = org_info["id"]
                logger.debug(
                    "extracted_organization_from_summary",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    organization_id=org_info["id"],
                )

        # Extract credential_type ID from summary_fields
        if "summary_fields" in data and "credential_type" in data["summary_fields"]:
            cred_type_info = data["summary_fields"]["credential_type"]
            if isinstance(cred_type_info, dict) and "id" in cred_type_info:
                data["credential_type"] = cred_type_info["id"]
                logger.debug(
                    "extracted_credential_type_from_summary",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    credential_type_id=cred_type_info["id"],
                )

        # Handle encrypted fields - mark them for special handling
        if "inputs" in data:
            encrypted_fields = []
            for key, value in list(data["inputs"].items()):
                if value == "$encrypted$":
                    encrypted_fields.append(key)

            if encrypted_fields:
                data["_needs_vault_lookup"] = True
                data["_encrypted_fields"] = encrypted_fields
                logger.debug(
                    "credential_has_encrypted_fields",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    encrypted_fields=encrypted_fields,
                )

            # Remove inputs with $encrypted$ values since they can't be imported
            data["inputs"] = {k: v for k, v in data["inputs"].items() if v != "$encrypted$"}

        return data

    def _transform_inventories(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply inventory-specific transformations.

        Note: Inventories with pending_deletion=true are filtered at the export
        phase and never reach the transformer. This method handles regular
        inventory transformations only.

        Args:
            data: Inventory data

        Returns:
            Transformed inventory data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Set inventory kind if not specified (default to empty string for regular inventory)
        if "kind" not in data:
            data["kind"] = ""

        # Ensure variables is a dict, not a string
        if "variables" in data and isinstance(data["variables"], str):
            import json

            try:
                data["variables"] = json.loads(data["variables"])
            except json.JSONDecodeError:
                logger.warning(
                    "failed_to_parse_variables",
                    resource_type="inventories",
                    source_id=source_id,
                    source_name=data.get("name"),
                    variables=data["variables"],
                )
                data["variables"] = {}

        return data

    def _transform_inventory_groups(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply inventory group-specific transformations.

        Extracts inventory ID from summary_fields before it's removed.

        Args:
            data: Inventory group data

        Returns:
            Transformed inventory group data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract inventory ID from summary_fields BEFORE they're removed
        if "summary_fields" in data and "inventory" in data["summary_fields"]:
            inv_info = data["summary_fields"]["inventory"]
            if isinstance(inv_info, dict) and "id" in inv_info:
                data["inventory"] = inv_info["id"]
                logger.debug(
                    "extracted_inventory_from_summary",
                    resource_type="inventory_groups",
                    source_id=source_id,
                    source_name=data.get("name"),
                    inventory_id=inv_info["id"],
                )

        return data

    def _transform_job_templates(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply job template-specific transformations.

        Args:
            data: Job template data

        Returns:
            Transformed job template data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Handle execution environment migration
        if "custom_virtualenv" in data and "execution_environment" not in data:
            # Mark that this needs an execution environment
            logger.warning(
                "job_template_uses_custom_virtualenv",
                resource_type="job_templates",
                source_id=source_id,
                source_name=data.get("name"),
                custom_virtualenv=data["custom_virtualenv"],
            )

        return data

    def _transform_projects(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply project-specific transformations.

        Extracts required fields from summary_fields before removal.

        Args:
            data: Project data

        Returns:
            Transformed project data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract organization ID from summary_fields (required)
        if "summary_fields" in data and "organization" in data["summary_fields"]:
            org_info = data["summary_fields"]["organization"]
            if isinstance(org_info, dict) and "id" in org_info:
                data["organization"] = org_info["id"]
                logger.debug(
                    "extracted_organization_from_summary",
                    resource_type="projects",
                    source_id=source_id,
                    source_name=data.get("name"),
                    organization_id=org_info["id"],
                )

        # Extract default_environment ID (handle missing gracefully)
        if "summary_fields" in data and "default_environment" in data["summary_fields"]:
            ee_info = data["summary_fields"]["default_environment"]
            if isinstance(ee_info, dict) and "id" in ee_info:
                data["default_environment"] = ee_info["id"]
                logger.debug(
                    "extracted_default_environment_from_summary",
                    resource_type="projects",
                    source_id=source_id,
                    source_name=data.get("name"),
                    ee_id=ee_info["id"],
                )
            elif ee_info is None:
                # Explicitly set to None if summary_fields says so
                data["default_environment"] = None

        # Extract credential ID (optional)
        if "summary_fields" in data and "credential" in data["summary_fields"]:
            cred_info = data["summary_fields"]["credential"]
            if isinstance(cred_info, dict) and "id" in cred_info:
                data["credential"] = cred_info["id"]
                logger.debug(
                    "extracted_credential_from_summary",
                    resource_type="projects",
                    source_id=source_id,
                    source_name=data.get("name"),
                    credential_id=cred_info["id"],
                )

        return data

    def _transform_inventory_sources(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply inventory source-specific transformations.

        Extracts required fields from summary_fields before removal.

        Args:
            data: Inventory source data

        Returns:
            Transformed inventory source data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract inventory ID (required)
        if "summary_fields" in data and "inventory" in data["summary_fields"]:
            inv_info = data["summary_fields"]["inventory"]
            if isinstance(inv_info, dict) and "id" in inv_info:
                data["inventory"] = inv_info["id"]
                logger.debug(
                    "extracted_inventory_from_summary",
                    resource_type="inventory_sources",
                    source_id=source_id,
                    source_name=data.get("name"),
                    inventory_id=inv_info["id"],
                )

        # Extract source_project ID (optional)
        if "summary_fields" in data and "source_project" in data["summary_fields"]:
            proj_info = data["summary_fields"]["source_project"]
            if isinstance(proj_info, dict) and "id" in proj_info:
                data["source_project"] = proj_info["id"]
                logger.debug(
                    "extracted_source_project_from_summary",
                    resource_type="inventory_sources",
                    source_id=source_id,
                    source_name=data.get("name"),
                    project_id=proj_info["id"],
                )

        # Extract credential ID (optional)
        if "summary_fields" in data and "credential" in data["summary_fields"]:
            cred_info = data["summary_fields"]["credential"]
            if isinstance(cred_info, dict) and "id" in cred_info:
                data["credential"] = cred_info["id"]
                logger.debug(
                    "extracted_credential_from_summary",
                    resource_type="inventory_sources",
                    source_id=source_id,
                    source_name=data.get("name"),
                    credential_id=cred_info["id"],
                )

        # Extract execution_environment ID (optional)
        if "summary_fields" in data and "execution_environment" in data["summary_fields"]:
            ee_info = data["summary_fields"]["execution_environment"]
            if isinstance(ee_info, dict) and "id" in ee_info:
                data["execution_environment"] = ee_info["id"]
                logger.debug(
                    "extracted_execution_environment_from_summary",
                    resource_type="inventory_sources",
                    source_id=source_id,
                    source_name=data.get("name"),
                    ee_id=ee_info["id"],
                )

        return data

    def _transform_execution_environments(self, data: dict[str, Any]) -> dict[str, Any]:
        """Apply execution environment-specific transformations.

        Extracts required fields from summary_fields before removal.

        Args:
            data: Execution environment data

        Returns:
            Transformed execution environment data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract organization ID (required)
        if "summary_fields" in data and "organization" in data["summary_fields"]:
            org_info = data["summary_fields"]["organization"]
            if isinstance(org_info, dict) and "id" in org_info:
                data["organization"] = org_info["id"]
                logger.debug(
                    "extracted_organization_from_summary",
                    resource_type="execution_environments",
                    source_id=source_id,
                    source_name=data.get("name"),
                    organization_id=org_info["id"],
                )

        # Extract credential ID (optional - for registry auth)
        if "summary_fields" in data and "credential" in data["summary_fields"]:
            cred_info = data["summary_fields"]["credential"]
            if isinstance(cred_info, dict) and "id" in cred_info:
                data["credential"] = cred_info["id"]
                logger.debug(
                    "extracted_credential_from_summary",
                    resource_type="execution_environments",
                    source_id=source_id,
                    source_name=data.get("name"),
                    credential_id=cred_info["id"],
                )

        return data

    def _validate_transformed_data(self, data: dict[str, Any], resource_type: str) -> None:
        """Validate transformed data meets AAP 2.6 requirements.

        Args:
            data: Transformed resource data
            resource_type: Type of resource

        Raises:
            ValueError: If validation fails
        """
        # Basic validation - ensure required fields exist
        required = self.REQUIRED_FIELDS.get(resource_type, {})
        source_id = data.get("_source_id") or data.get("id")

        for field in required.keys():
            if field not in data:
                logger.error(
                    "validation_failed_missing_field",
                    resource_type=resource_type,
                    source_id=source_id,
                    source_name=data.get("name"),
                    field=field,
                )
                self.stats["validation_errors"] += 1
                raise ValueError(
                    f"Required field '{field}' missing in {resource_type} after transformation"
                )

    def get_stats(self) -> dict[str, int]:
        """Get transformation statistics.

        Returns:
            Dictionary with transformation statistics
        """
        return self.stats.copy()

    def reset_stats(self) -> None:
        """Reset transformation statistics."""
        self.stats = {
            "transformed_count": 0,
            "fields_removed": 0,
            "fields_added": 0,
            "validation_errors": 0,
        }


class InventoryTransformer(DataTransformer):
    """Transformer for inventory resources."""

    DEPENDENCIES = {
        "organization": "organizations",
    }
    REQUIRED_DEPENDENCIES = {"organization"}

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply inventory-specific transformations.

        Args:
            data: Inventory data
            resource_type: Resource type (should be 'inventories')

        Returns:
            Transformed inventory data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Set inventory kind if not specified (default to empty string for regular inventory)
        if "kind" not in data:
            data["kind"] = ""

        # AAP 2.6 API requires variables field as JSON string, not dict
        if "variables" in data:
            if isinstance(data["variables"], dict):
                # Convert dict to JSON string
                data["variables"] = json.dumps(data["variables"])
            elif not isinstance(data["variables"], str):
                # Handle unexpected types (None, list, etc.) - convert to empty JSON object
                logger.warning(
                    "unexpected_variables_type",
                    resource_type="inventories",
                    source_id=source_id,
                    source_name=data.get("name"),
                    variables_type=type(data["variables"]).__name__,
                )
                data["variables"] = "{}"
            # If already a string, leave as-is (already in correct format)

        return data


class CredentialTransformer(DataTransformer):
    """Transformer for credential resources."""

    DEPENDENCIES = {
        "organization": "organizations",
        "credential_type": "credential_types",
        "user": "users",
        "team": "teams",
    }
    # Organization and credential_type are required; user/team ownership is optional
    REQUIRED_DEPENDENCIES = {"organization", "credential_type"}

    def __init__(self, *args: Any, **kwargs: Any):
        super().__init__(*args, **kwargs)
        self.external_credential_type_ids: set[int] | None = None

    def _load_external_credential_types(self) -> None:
        """Load IDs of external credential types from export files."""
        self.external_credential_type_ids = set()
        if not self.input_dir:
            return

        cred_types_dir = self.input_dir / "credential_types"
        if not cred_types_dir.exists():
            return

        for json_file in cred_types_dir.glob("credential_types_*.json"):
            try:
                with open(json_file) as f:
                    types = json.load(f)
                    for ct in types:
                        if ct.get("kind") == "external":
                            # Use _source_id if available, otherwise id
                            source_id = ct.get("_source_id") or ct.get("id")
                            if source_id:
                                self.external_credential_type_ids.add(source_id)
            except Exception as e:
                logger.warning(
                    "failed_to_load_external_credential_types",
                    file=str(json_file),
                    error=str(e),
                )

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply credential-specific transformations.

        Args:
            data: Credential data
            resource_type: Resource type (should be 'credentials')

        Returns:
            Transformed credential data
        """
        # Load external credential types if not already loaded
        if self.external_credential_type_ids is None:
            self._load_external_credential_types()

        source_id = data.get("_source_id") or data.get("id")

        # Check if credential depends on an external credential type that is NOT mapped
        cred_type_id = data.get("credential_type")
        if cred_type_id and self.external_credential_type_ids and cred_type_id in self.external_credential_type_ids:
            # Check if mapped
            if self.state and not self.state.get_mapped_id("credential_types", cred_type_id):
                logger.info(
                    "skipping_credential_external_type_unmapped",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    credential_type_id=cred_type_id,
                    message="Credential depends on external credential type that does not exist in target",
                )
                self.stats["skipped_count"] += 1
                raise SkipResourceError(
                    f"Credential {source_id} depends on unmapped external credential type {cred_type_id}",
                    resource_type=resource_type,
                    source_id=source_id,
                    missing_dependency=f"credential_types:{cred_type_id}",
                )

        # Extract organization ID from summary_fields BEFORE they're removed
        if "summary_fields" in data and "organization" in data["summary_fields"]:
            org_info = data["summary_fields"]["organization"]
            if isinstance(org_info, dict) and "id" in org_info:
                data["organization"] = org_info["id"]
                logger.debug(
                    "extracted_organization_from_summary",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    organization_id=org_info["id"],
                )

        # Extract credential_type ID from summary_fields
        if "summary_fields" in data and "credential_type" in data["summary_fields"]:
            cred_type_info = data["summary_fields"]["credential_type"]
            if isinstance(cred_type_info, dict) and "id" in cred_type_info:
                data["credential_type"] = cred_type_info["id"]
                logger.debug(
                    "extracted_credential_type_from_summary",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    credential_type_id=cred_type_info["id"],
                )

        # VALIDATION: Ensure credential_type is present (REQUIRED field in AAP 2.6)
        if "credential_type" not in data or data.get("credential_type") is None:
            # Attempt recovery from related.credential_type URL
            if "related" in data and "credential_type" in data["related"]:
                import re

                cred_type_url = data["related"]["credential_type"]
                # Extract ID from URL like "/api/v2/credential_types/18/"
                match = re.search(r"/credential_types/(\d+)/", cred_type_url)
                if match:
                    data["credential_type"] = int(match.group(1))
                    logger.warning(
                        "credential_type_recovered_from_related",
                        resource_type="credentials",
                        source_id=source_id,
                        source_name=data.get("name"),
                        credential_type_id=data["credential_type"],
                        message="credential_type recovered from related URL",
                    )
                else:
                    logger.error(
                        "credential_missing_credential_type",
                        resource_type="credentials",
                        source_id=source_id,
                        source_name=data.get("name"),
                        related_url=cred_type_url,
                        message="credential_type field is REQUIRED but missing and could not be recovered - import will fail",
                    )
            else:
                logger.error(
                    "credential_missing_credential_type",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    message="credential_type field is REQUIRED but missing - import will fail",
                )

        # Handle encrypted fields - generate temporary values so creation succeeds
        if "inputs" in data:
            temp_values = {}
            encrypted_fields = []

            ssh_key_fields = {"ssh_key_data", "private_key", "ssh_private_key"}

            # First pass: Check if we need an encrypted key (ssh_key_unlock is present/encrypted)
            ssh_key_unlock_value = None
            if "ssh_key_unlock" in data["inputs"] and data["inputs"]["ssh_key_unlock"] == "$encrypted$":
                # Generate a passphrase first
                ssh_key_unlock_value = secrets.token_urlsafe(16)
                data["inputs"]["ssh_key_unlock"] = ssh_key_unlock_value
                temp_values["ssh_key_unlock"] = ssh_key_unlock_value
                encrypted_fields.append("ssh_key_unlock")

            for key, value in data["inputs"].items():
                if value == "$encrypted$":
                    # Skip if already handled (e.g. ssh_key_unlock)
                    if key == "ssh_key_unlock":
                        continue

                    if key in ssh_key_fields or ("private" in key.lower() and "key" in key.lower()):
                        # Generate valid PEM format for SSH key fields
                        if ssh_key_unlock_value:
                            # Use encrypted key generator with the passphrase we generated
                            temp_value = generate_temp_encrypted_ssh_key(ssh_key_unlock_value)
                        else:
                            # Use unencrypted key generator
                            temp_value = generate_temp_ssh_key()
                    else:
                        # Generate random temp value (16 chars) for other secrets
                        temp_value = secrets.token_urlsafe(16)

                    data["inputs"][key] = temp_value
                    temp_values[key] = temp_value
                    encrypted_fields.append(key)

            if temp_values:
                data["_needs_vault_lookup"] = True
                data["_encrypted_fields"] = encrypted_fields
                data["_temp_credential_values"] = temp_values
                logger.info(
                    "credential_temp_values_generated",
                    resource_type="credentials",
                    source_id=source_id,
                    source_name=data.get("name"),
                    fields=list(temp_values.keys()),
                    message="Temporary values generated for encrypted fields - update after migration",
                )

        # Set null organization to Default (ID=1) for API compatibility
        # Testing if organization is required by the API
        if "organization" in data and data["organization"] is None:
            data["organization"] = 1  # Default organization
            logger.info(
                "defaulted_null_organization",
                resource_type="credentials",
                source_id=source_id,
                source_name=data.get("name"),
                organization_id=1,
                message="Set null organization to Default (ID=1)",
            )

        return data

    async def populate_target_id_from_target(
        self,
        data: dict[str, Any],
        target_client: Any,  # AAPTargetClient - avoid circular import
        state: MigrationState,
        source_id: int,
    ) -> dict[str, Any]:
        """Skip lookup for credentials - they will be created during import.

        We no longer pre-populate credential mappings because we support creating
        credentials with temporary values if they don't exist.
        """
        self.stats["skipped_count"] += 1 # Increment skipped count for this item
        return data

    # Old implementation commented out/replaced
    # async def populate_target_id_from_target_OLD(...):


class JobTemplateTransformer(DataTransformer):
    """Transformer for job template resources."""

    DEPENDENCIES = {
        "organization": "organizations",
        "inventory": "inventories",
        "project": "projects",
        "credential": "credentials",
        "execution_environment": "execution_environments",
    }
    # Organization and project are required; inventory/credential/EE are optional
    REQUIRED_DEPENDENCIES = {"organization", "project"}

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply job template-specific transformations.

        Args:
            data: Job template data
            resource_type: Resource type (should be 'job_templates')

        Returns:
            Transformed job template data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Handle execution environment migration
        if "custom_virtualenv" in data and "execution_environment" not in data:
            # Mark that this needs an execution environment
            data["_needs_execution_environment"] = True
            data["_custom_virtualenv_path"] = data["custom_virtualenv"]
            logger.debug(
                "job_template_needs_ee_mapping",
                resource_type="job_templates",
                source_id=source_id,
                source_name=data.get("name"),
                virtualenv=data["custom_virtualenv"],
            )
            # Remove deprecated custom_virtualenv field
            del data["custom_virtualenv"]

        # Fallback: Extract credentials from summary_fields if not explicitly exported
        # This handles cases where the exporter didn't populate credentials (e.g. raw export)
        if "credentials" not in data and "summary_fields" in data:
            summary = data["summary_fields"]
            if "credentials" in summary:
                creds = summary["credentials"]
                if isinstance(creds, list):
                    # Extract full credential details from list of credential summaries
                    extracted_creds = []
                    for c in creds:
                        if isinstance(c, dict) and "id" in c:
                            extracted_creds.append({
                                "id": c["id"],
                                "name": c.get("name"),
                                "description": c.get("description"),
                                "kind": c.get("kind"),
                                "cloud": c.get("cloud", False),
                            })

                    if extracted_creds:
                        data["credentials"] = extracted_creds
                        logger.debug(
                            "extracted_credentials_from_summary",
                            resource_type="job_templates",
                            source_id=source_id,
                            credential_count=len(extracted_creds),
                            source="summary_fields",
                        )

        # Remove _credentials field as it's replaced by credentials with full details
        if "_credentials" in data:
            del data["_credentials"]

        # Ensure boolean fields are proper booleans
        boolean_fields = [
            "allow_simultaneous",
            "ask_variables_on_launch",
            "ask_tags_on_launch",
            "ask_skip_tags_on_launch",
            "ask_job_type_on_launch",
            "ask_verbosity_on_launch",
            "ask_inventory_on_launch",
            "ask_credential_on_launch",
            "ask_diff_mode_on_launch",
            "ask_limit_on_launch",
        ]

        for field in boolean_fields:
            if field in data and not isinstance(data[field], bool):
                data[field] = self._to_bool(data[field])

        return data

    @staticmethod
    def _to_bool(value: Any) -> bool:
        """Convert value to boolean, handling string "true"/"false".

        Args:
            value: Value to convert

        Returns:
            Boolean value
        """
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ("true", "1", "yes", "y", "on")
        return bool(value)


class ProjectTransformer(DataTransformer):
    """Transformer for project resources."""

    DEPENDENCIES = {
        "organization": "organizations",
        "credential": "credentials",
    }
    # Organization is required; credential (for SCM auth) is optional
    REQUIRED_DEPENDENCIES = {"organization"}

    def __init__(self, *args: Any, defer_project_sync: bool = True, **kwargs: Any):
        """Initialize project transformer.

        Args:
            *args: Positional arguments for DataTransformer
            defer_project_sync: Whether to defer SCM sync by stripping SCM details
            **kwargs: Keyword arguments for DataTransformer
        """
        super().__init__(*args, **kwargs)
        self.defer_project_sync = defer_project_sync

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply project-specific transformations.

        Args:
            data: Project data
            resource_type: Resource type (should be 'projects')

        Returns:
            Transformed project data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Ensure scm_update_on_launch is boolean
        if "scm_update_on_launch" in data and not isinstance(data["scm_update_on_launch"], bool):
            data["scm_update_on_launch"] = bool(data["scm_update_on_launch"])

        # Ensure scm_update_cache_timeout is integer
        if "scm_update_cache_timeout" in data:
            try:
                data["scm_update_cache_timeout"] = int(data["scm_update_cache_timeout"])
            except (ValueError, TypeError):
                logger.warning(
                    "invalid_scm_cache_timeout",
                    resource_type="projects",
                    source_id=source_id,
                    source_name=data.get("name"),
                    value=data["scm_update_cache_timeout"],
                )
                data["scm_update_cache_timeout"] = 0

        # Handle deferred sync (Phase 2 preparation)
        if self.defer_project_sync:
            scm_type = data.get("scm_type")
            # Only defer if it's an SCM project (git, svn, archive) and not manual ("")
            if scm_type and scm_type in ("git", "svn", "archive"):
                # Move SCM details to deferred field
                data["_deferred_scm_details"] = {
                    "scm_type": scm_type,
                    "scm_url": data.get("scm_url"),
                    "scm_branch": data.get("scm_branch", ""),
                    "scm_clean": data.get("scm_clean", False),
                    "scm_delete_on_update": data.get("scm_delete_on_update", False),
                    "scm_update_on_launch": data.get("scm_update_on_launch", False),
                    "scm_update_cache_timeout": data.get("scm_update_cache_timeout", 0),
                    "credential": data.get("credential"),  # Keep source credential ID
                }

                # Clear SCM details from main payload to prevent sync
                data["scm_type"] = ""  # Manual
                data["scm_url"] = ""
                # Remove other SCM fields to be clean
                for field in [
                    "scm_branch", "scm_clean", "scm_delete_on_update",
                    "scm_update_on_launch", "scm_update_cache_timeout", "credential"
                ]:
                    data.pop(field, None)

                logger.debug(
                    "project_scm_sync_deferred",
                    source_id=source_id,
                    name=data.get("name"),
                    original_type=scm_type,
                )

        return data


class WorkflowTransformer(DataTransformer):
    """Transformer for workflow job template resources."""

    DEPENDENCIES = {
        "organization": "organizations",
        "inventory": "inventories",
    }
    # Organization is required; inventory is optional
    REQUIRED_DEPENDENCIES = {"organization"}

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply workflow-specific transformations.

        Args:
            data: Workflow data
            resource_type: Resource type (should be 'workflow_job_templates')

        Returns:
            Transformed workflow data
        """
        # Remove workflow nodes - they need to be handled separately
        if "nodes" in data:
            # Store nodes separately for later processing
            data["_workflow_nodes"] = data.pop("nodes")

        # Ensure boolean fields are proper booleans
        boolean_fields = ["ask_variables_on_launch", "ask_inventory_on_launch", "survey_enabled"]

        for field in boolean_fields:
            if field in data and not isinstance(data[field], bool):
                data[field] = self._to_bool(data[field])

        return data

    @staticmethod
    def _to_bool(value: Any) -> bool:
        """Convert value to boolean, handling string "true"/"false".

        Args:
            value: Value to convert

        Returns:
            Boolean value
        """
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() in ("true", "1", "yes", "y", "on")
        return bool(value)


# =============================================================================
# Additional Resource-Specific Transformers
# =============================================================================


class OrganizationTransformer(DataTransformer):
    """Transformer for organization resources.

    Organizations are the root dependency - they have no dependencies themselves.
    """

    DEPENDENCIES = {}
    REQUIRED_DEPENDENCIES = set()


class InstanceGroupTransformer(DataTransformer):
    """Transformer for instance_group resources.

    Transforms instance hostnames in policy_instance_list using mappings
    from config/mappings.yaml to handle different hostnames between environments.
    """

    DEPENDENCIES = {}  # No dependencies - instance_groups reference instances by hostname
    REQUIRED_DEPENDENCIES: set[str] = set()

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply instance_group data-specific transformations.

        Maps hostnames in policy_instance_list using config/mappings.yaml.
        """
        # No super() call here, as this is a specific transformation hook.

        # Get instance hostname mappings from config
        instance_mappings: dict[str, str] = {}
        if self.config and hasattr(self.config, "resource_mappings"):
            instance_mappings = self.config.resource_mappings.get("instances", {})

        # Transform policy_instance_list hostnames
        if "policy_instance_list" in data and data["policy_instance_list"]:
            original_list = data["policy_instance_list"]
            data["policy_instance_list"] = [
                instance_mappings.get(hostname, hostname)
                for hostname in original_list
            ]
            if original_list != data["policy_instance_list"]:
                logger.info(
                    "instance_group_hostnames_mapped",
                    original=original_list,
                    mapped=data["policy_instance_list"],
                )

        return data


class HostTransformer(DataTransformer):
    """Transformer for host resources with inventory validation."""

    DEPENDENCIES = {
        "inventory": "inventories",
    }
    # Hosts MUST have a valid inventory
    REQUIRED_DEPENDENCIES = {"inventory"}

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply host-specific transformations.

        Args:
            data: Host data
            resource_type: Resource type (should be 'hosts')

        Returns:
            Transformed host data
        """
        # Convert variables to JSON string if needed
        if "variables" in data:
            if isinstance(data["variables"], dict):
                data["variables"] = json.dumps(data["variables"])
            elif not isinstance(data["variables"], str):
                data["variables"] = "{}"

        return data


class LabelTransformer(DataTransformer):
    """Transformer for label resources."""

    DEPENDENCIES = {
        "organization": "organizations",
    }
    REQUIRED_DEPENDENCIES = {"organization"}


class TeamTransformer(DataTransformer):
    """Transformer for team resources."""

    DEPENDENCIES = {
        "organization": "organizations",
    }
    # Organization is optional for teams (some teams may be global)
    REQUIRED_DEPENDENCIES = set()


class CredentialTypeTransformer(DataTransformer):
    """Transformer for credential type resources."""

    DEPENDENCIES = {
        "organization": "organizations",
    }
    # Organization is optional - built-in types (ID 1-27) have no organization
    REQUIRED_DEPENDENCIES = set()

    def _get_name_mapping(self, name: str) -> str:
        """Get mapped name from configuration."""
        if not self.config or not name:
            return name

        mappings = self.config.resource_mappings.get("credential_types", {})
        return mappings.get(name, name)

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply credential type-specific transformations.

        Args:
            data: Credential type data
            resource_type: Resource type (should be 'credential_types')

        Returns:
            Transformed credential type data
        """
        # Built-in credential types are managed by the system
        if data.get("managed"):
            data["_is_builtin"] = True

            # Map legacy names to new names (e.g. CyberArk)
            name = data.get("name")
            new_name = self._get_name_mapping(name)

            if name != new_name:
                logger.info(
                    "renaming_credential_type",
                    source_id=data.get("_source_id") or data.get("id"),
                    old_name=name,
                    new_name=new_name,
                )
                data["name"] = new_name

            logger.debug(
                "builtin_credential_type",
                resource_type="credential_types",
                source_id=data.get("_source_id") or data.get("id"),
                source_name=data.get("name"),
                message="Built-in credential type (managed=True)",
            )

        # Clean up inputs schema - remove 'metadata' if present (causes validation errors in 2.6)
        # Error: Additional properties are not allowed ('metadata' was unexpected)
        inputs = data.get("inputs")
        if isinstance(inputs, dict) and "metadata" in inputs:
            logger.debug(
                "removing_metadata_from_inputs",
                resource_type="credential_types",
                source_id=data.get("_source_id") or data.get("id"),
                source_name=data.get("name"),
            )
            del inputs["metadata"]

        return data

    async def populate_target_id_from_target(
        self,
        data: dict[str, Any],
        target_client: Any,  # AAPTargetClient - avoid circular import
        state: MigrationState,
        source_id: int,
    ) -> dict[str, Any]:
        """Query target environment to find matching credential_type by name.

        Only looks up built-in credential types (managed=True). Custom types are
        created during import if they don't exist.

        Args:
            data: Credential type data (with name field)
            target_client: AAP target client instance
            state: Migration state manager for saving mappings
            source_id: Source credential_type ID

        Returns:
            Data dict (unchanged, mapping saved to state)
        """
        # Skip custom types (managed=False) - they will be created during import
        if not data.get("managed"):
            self.stats["skipped_count"] += 1 # Increment skipped count for this item
            return data

        name = data.get("name")
        if not name:
            logger.warning(
                "credential_type_missing_name",
                source_id=source_id,
                message="Cannot map credential_type without name",
            )
            return data

        # Map name if necessary (handle 2.3 -> 2.6 name changes)
        lookup_name = self._get_name_mapping(name)
        if lookup_name != name:
            logger.info(
                "credential_type_name_mapped",
                source_id=source_id,
                original_name=name,
                mapped_name=lookup_name,
            )

        try:
            # Query target for credential_type with matching name
            results = await target_client.get(
                "credential_types/",
                params={"name": lookup_name},
            )

            resources = results.get("results", [])
            if resources and len(resources) > 0:
                target_id = resources[0]["id"]
                state.mark_completed("credential_types", source_id, target_id, source_name=name)
                logger.info(
                    "credential_type_mapped_from_target",
                    name=name,
                    lookup_name=lookup_name,
                    source_id=source_id,
                    target_id=target_id,
                )
            else:
                logger.warning(
                    "credential_type_not_found_in_target",
                    name=name,
                    lookup_name=lookup_name,
                    source_id=source_id,
                    message="Credential type not pre-created in target - dependent resources may fail",
                )

        except Exception as e:
            logger.error(
                "credential_type_target_lookup_failed",
                name=name,
                source_id=source_id,
                error=str(e),
            )

        return data


class ExecutionEnvironmentTransformer(DataTransformer):
    """Transformer for execution environment resources."""

    DEPENDENCIES = {
        "organization": "organizations",
        "credential": "credentials",
    }
    # Organization is required; credential (for private registries) is optional
    REQUIRED_DEPENDENCIES = {"organization"}


class InventoryGroupTransformer(DataTransformer):
    """Transformer for inventory group resources."""

    DEPENDENCIES = {
        "inventory": "inventories",
    }
    REQUIRED_DEPENDENCIES = {"inventory"}

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply inventory group-specific transformations.

        Args:
            data: Inventory group data
            resource_type: Resource type (should be 'inventory_groups' or 'groups')

        Returns:
            Transformed inventory group data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract inventory ID from summary_fields if not already set
        if "inventory" not in data and "summary_fields" in data:
            if "inventory" in data["summary_fields"]:
                inv_info = data["summary_fields"]["inventory"]
                if isinstance(inv_info, dict) and "id" in inv_info:
                    data["inventory"] = inv_info["id"]
                    logger.debug(
                        "extracted_inventory_from_summary",
                        resource_type="inventory_groups",
                        source_id=source_id,
                        source_name=data.get("name"),
                        inventory_id=inv_info["id"],
                    )

        # Convert variables to JSON string if needed
        if "variables" in data:
            if isinstance(data["variables"], dict):
                data["variables"] = json.dumps(data["variables"])
            elif not isinstance(data["variables"], str):
                data["variables"] = "{}"

        return data


class InventorySourceTransformer(DataTransformer):
    """Transformer for inventory source resources."""

    DEPENDENCIES = {
        "inventory": "inventories",
        "source_project": "projects",
        "credential": "credentials",
        "execution_environment": "execution_environments",
    }
    # Inventory is required; source_project/credential/EE are optional
    REQUIRED_DEPENDENCIES = {"inventory"}

    def _apply_specific_transformations(
        self, data: dict[str, Any], resource_type: str
    ) -> dict[str, Any]:
        """Apply inventory source-specific transformations.

        Args:
            data: Inventory source data
            resource_type: Resource type (should be 'inventory_sources')

        Returns:
            Transformed inventory source data
        """
        source_id = data.get("_source_id") or data.get("id")

        # Extract inventory ID from summary_fields if not already set
        if "inventory" not in data and "summary_fields" in data:
            if "inventory" in data["summary_fields"]:
                inv_info = data["summary_fields"]["inventory"]
                if isinstance(inv_info, dict) and "id" in inv_info:
                    data["inventory"] = inv_info["id"]
                    logger.debug(
                        "extracted_inventory_from_summary",
                        resource_type="inventory_sources",
                        source_id=source_id,
                        source_name=data.get("name"),
                        inventory_id=inv_info["id"],
                    )

        # Extract source_project ID from summary_fields
        if "source_project" not in data and "summary_fields" in data:
            if "source_project" in data["summary_fields"]:
                proj_info = data["summary_fields"]["source_project"]
                if isinstance(proj_info, dict) and "id" in proj_info:
                    data["source_project"] = proj_info["id"]

        return data


class ScheduleTransformer(DataTransformer):
    """Transformer for schedule resources."""

    DEPENDENCIES = {
        "unified_job_template": "unified_job_templates",
    }
    REQUIRED_DEPENDENCIES = {"unified_job_template"}

    def _validate_dependencies(
        self,
        data: dict[str, Any],
        resource_type: str,
    ) -> None:
        """Validate unified_job_template dependency polymorphically.

        Schedules depend on a 'unified_job_template' which can be one of:
        - job_template
        - workflow_job_template
        - project
        - inventory_source
        - system_job_template (not supported)

        We need to determine the actual type from summary_fields to look up
        the correct ID mapping.
        """
        if not self.state:
            return

        source_id = data.get("_source_id") or data.get("id")
        ujt_id = data.get("unified_job_template")

        if not ujt_id:
            # Should be caught by base validation, but safe to check
            return

        # Determine the actual resource type of the UJT
        ujt_type = None

        # Try to get type from summary_fields
        if "summary_fields" in data and "unified_job_template" in data["summary_fields"]:
            ujt_summary = data["summary_fields"]["unified_job_template"]
            # AAP API returns 'unified_job_template' object with 'type' field
            # Types: 'job_template', 'workflow_job_template', 'project', 'inventory_source', etc.
            # Note: Some older AAP versions might use 'unified_job_template' as type name in some contexts,
            # but usually the specific type is available.

            # In summary_fields, we look for the object representing the UJT
            if isinstance(ujt_summary, dict):
                # The 'type' field in the summary object tells us the resource type
                # e.g. "job_template", "workflow_job_template"
                # Mappings in migration tool match these API type names (mostly pluralized in state, but check singular)

                # API types are singular (job_template), our state uses plural (job_templates)
                api_type = ujt_summary.get("type")

                # Fallback for system jobs where 'type' might be missing but 'unified_job_type' exists
                if not api_type and ujt_summary.get("unified_job_type") == "system_job":
                    api_type = "system_job_template"

                if api_type:
                    # Map API type to our internal resource type (usually plural)
                    type_map = {
                        "job_template": "job_templates",
                        "workflow_job_template": "workflow_job_templates",
                        "project": "projects",
                        "inventory_source": "inventory_sources",
                        "system_job_template": "system_job_templates", # Not migrated usually
                    }
                    ujt_type = type_map.get(api_type)

        if not ujt_type:
            # Fallback: Try to infer type or skip if unknown
            # If we can't determine type, we can't validate dependency correctly
            logger.warning(
                "schedule_unknown_ujt_type",
                source_id=source_id,
                source_name=data.get("name"),
                message="Could not determine type of unified_job_template from summary_fields",
            )
            # We will try to proceed, but might fail later.
            # For now, let's assume if we can't find it, we skip it to be safe?
            # Or maybe we just try 'job_templates' as default?
            # Better to skip if we can't validate.
            self.stats["skipped_count"] += 1
            raise SkipResourceError(
                f"Schedule {source_id} has unknown unified_job_template type",
                resource_type=resource_type,
                source_id=source_id,
                missing_dependency="unified_job_template:unknown_type",
            )

        # Validate dependency exists in state
        if not self.state.has_source_mapping(ujt_type, ujt_id):
            logger.warning(
                "schedule_dependency_missing",
                source_id=source_id,
                source_name=data.get("name"),
                ujt_type=ujt_type,
                ujt_id=ujt_id,
                message=f"Skipping schedule - required {ujt_type} {ujt_id} not exported",
            )
            self.stats["skipped_count"] += 1
            raise SkipResourceError(
                f"Schedule {source_id} references non-exported {ujt_type} {ujt_id}",
                resource_type=resource_type,
                source_id=source_id,
                missing_dependency=f"{ujt_type}:{ujt_id}",
            )

        # If valid, we might want to store the resolved type for the importer to use?
        # The importer will need to know which table to look up.
        # We can add a temporary field to data.
        data["_ujt_resource_type"] = ujt_type


class SystemJobTemplateTransformer(DataTransformer):
    """Transformer for system job template resources.

    System job templates (e.g. Cleanup jobs) exist in both environments.
    We map them by name/ID.
    """
    DEPENDENCIES = {}
    REQUIRED_DEPENDENCIES = set()

    async def populate_target_id_from_target(
        self,
        data: dict[str, Any],
        target_client: Any,
        state: MigrationState,
        source_id: int,
    ) -> dict[str, Any]:
        """Map system job templates by name in target."""
        name = data.get("name")
        if not name:
            return data

        try:
            # Lookup by name
            results = await target_client.get(
                "system_job_templates/",
                params={"name": name},
            )
            resources = results.get("results", [])

            if resources:
                target_id = resources[0]["id"]
                state.mark_completed("system_job_templates", source_id, target_id, source_name=name)
                logger.info(
                    "system_job_template_mapped",
                    source_id=source_id,
                    target_id=target_id,
                    name=name,
                )
            else:
                logger.warning(
                    "system_job_template_not_found",
                    name=name,
                    source_id=source_id,
                )
        except Exception as e:
            logger.error(
                "system_job_template_lookup_failed",
                name=name,
                error=str(e),
            )

        return data


class NotificationTemplateTransformer(DataTransformer):
    """Transformer for notification template resources."""

    DEPENDENCIES = {
        "organization": "organizations",
    }
    REQUIRED_DEPENDENCIES = {"organization"}


class CredentialInputSourceTransformer(DataTransformer):
    """Transformer for credential input source resources.

    These resources link one credential's input field to another credential.
    They depend on both the "parent" credential and the "source" credential.
    """
    DEPENDENCIES = {
        "credential": "credentials",  # The credential being modified
        "source_credential": "credentials",  # The credential providing the input
    }
    REQUIRED_DEPENDENCIES = {"credential", "source_credential"}


# =============================================================================
# Transformer Factory
# =============================================================================

# Registry of all transformer classes
TRANSFORMER_CLASSES: dict[str, type[DataTransformer]] = {
    "organizations": OrganizationTransformer,
    "instances": DataTransformer,  # No dependencies - instances are foundational
    "instance_groups": InstanceGroupTransformer,
    "labels": LabelTransformer,
    "users": DataTransformer,  # No dependencies
    "teams": TeamTransformer,
    "credential_types": CredentialTypeTransformer,
    "credentials": CredentialTransformer,
    "execution_environments": ExecutionEnvironmentTransformer,
    "inventories": InventoryTransformer,
    "hosts": HostTransformer,
    "inventory_groups": InventoryGroupTransformer,
    "groups": InventoryGroupTransformer,  # Alias
    "inventory_sources": InventorySourceTransformer,
    "projects": ProjectTransformer,
    "job_templates": JobTemplateTransformer,
    "workflow_job_templates": WorkflowTransformer,
    "schedules": ScheduleTransformer,
    "system_job_templates": SystemJobTemplateTransformer,
    "notification_templates": NotificationTemplateTransformer,
    "credential_input_sources": CredentialInputSourceTransformer,
}


def create_transformer(
    resource_type: str,
    dry_run: bool = False,
    schema_comparison: ComparisonResult | None = None,
    schema_comparison_file: str | Path | None = None,
    state: MigrationState | None = None,
    input_dir: Path | None = None,
    config: MigrationConfig | None = None,
    **kwargs: Any,
) -> DataTransformer:
    """Create appropriate transformer for resource type.

    Args:
        resource_type: Type of resource (e.g., 'organizations')
        dry_run: Whether to run in dry-run mode
        schema_comparison: Optional schema comparison result
        schema_comparison_file: Optional path to schema comparison file
        state: Optional migration state for dependency validation
        input_dir: Optional input directory (RAW exports)
        config: Optional migration configuration containing resource mappings
        **kwargs: Additional arguments passed to transformer constructor

    Returns:
        Instance of appropriate DataTransformer subclass
    """
    transformer_cls = TRANSFORMER_CLASSES.get(resource_type, DataTransformer)

    return transformer_cls(
        dry_run=dry_run,
        schema_comparison=schema_comparison,
        schema_comparison_file=schema_comparison_file,
        state=state,
        input_dir=input_dir,
        config=config,
        **kwargs,
    )
